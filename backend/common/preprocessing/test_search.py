# import faiss
# import numpy as np
# import json
# from sentence_transformers import SentenceTransformer
# import time

# # --- Timer start for lasting ---
# start_load = time.time()

# # Last inn FAISS og metadata
# index = faiss.read_index("laws.index")
# with open("metadata.json", "r", encoding="utf-8") as f:
#     metadata = json.load(f)

# # Last modellen EN gang
# model = SentenceTransformer("all-MiniLM-L6-v2")

# end_load = time.time()
# print(f"‚è±Ô∏è Lasting av modell og FAISS tok: {end_load - start_load:.2f} sekunder")

# # --- S√∏ke-funksjon med timer ---
# def s√∏k_lov(query, k=3):
#     start_search = time.time()

#     # Embed sp√∏rring
#     query_vec = model.encode([query], convert_to_numpy=True)
#     query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)
#     query_vec = query_vec.astype("float32")  # FAISS bruker float32

#     # FAISS-s√∏k
#     D, I = index.search(query_vec, k)

#     end_search = time.time()
#     print(f"‚è±Ô∏è S√∏ket tok: {end_search - start_search:.4f} sekunder")

#     results = []
#     for i in I[0]:
#         doc = metadata[i]
#         print(f"\nüîé {doc['titleShort']} ‚Äî {doc['chapter']}")
#         print(doc['text'][:250], "...\n")
#         results.append(doc)
#     return results

# # --- Eksempel ---
# s√∏k_lov("Personvern")
# s√∏k_lov("DPIA")
# s√∏k_lov("GDPR")
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
import time

# --- Timer start for lasting ---
start_load = time.time()

# Last inn FAISS og metadata
index = faiss.read_index("laws.index")
with open("metadata.json", "r", encoding="utf-8") as f:
    metadata = json.load(f)

# Last modellen EN gang
model = SentenceTransformer("all-MiniLM-L6-v2")

end_load = time.time()
print(f"‚è±Ô∏è Lasting av modell og FAISS tok: {end_load - start_load:.2f} sekunder")

# --- S√∏ke-funksjon med lagring til txt ---
def s√∏k_lov(query, k=3, filnavn="s√∏k_resultater.txt"):
    start_search = time.time()

    # Embed sp√∏rring
    query_vec = model.encode([query], convert_to_numpy=True)
    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)
    query_vec = query_vec.astype("float32")  # FAISS bruker float32

    # FAISS-s√∏k
    D, I = index.search(query_vec, k)  # D = similarity score, I = indeks

    end_search = time.time()
    s√∏ketid = end_search - start_search
    print(f"‚è±Ô∏è S√∏ket tok: {s√∏ketid:.4f} sekunder")

    results = []
    
    # --- Skriv til fil ---
    with open(filnavn, "a", encoding="utf-8") as f:
        print(f"‚è±Ô∏è Lasting av modell og FAISS tok: {end_load - start_load:.2f} sekunder")
        f.write(f"\n===== S√∏keresultater for: '{query}' =====\n")
        f.write(f"‚è±Ô∏è S√∏ket tok: {s√∏ketid:.4f} sekunder\n\n")
        
        for idx, i in enumerate(I[0]):
            doc = metadata[i]
            score = D[0][idx]  # similarity score
            
            # Print til konsoll
            print(f"\nüîé {doc['titleShort']} ‚Äî {doc['chapter']}")
            print(f"Similarity score: {score:.4f}")
            print(doc['text'][:2500], "...\n")
            
            # Skriv til fil
            f.write(f"üîé {doc['titleShort']} ‚Äî {doc['chapter']}\n")
            f.write(f"Similarity score: {score:.4f}\n")
            f.write(doc['text'][:10000] + "...\n\n")  # lagre opptil 1000 tegn per tekst
            
            results.append({"doc": doc, "score": score})
    
    return results


# --- Eksempler ---
s√∏k_lov("Personvern datatilsynet internkontroll")
s√∏k_lov("rett til innsyn databehandler")
s√∏k_lov("den registrerte form√•l retting og sletting")
s√∏k_lov("student retningslinjer reservert studieplass")