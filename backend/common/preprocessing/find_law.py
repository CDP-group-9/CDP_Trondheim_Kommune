# import numpy as np
# import json
# import faiss
# from sentence_transformers import SentenceTransformer
# import time

# # --- LASTING AV DATA √âN GANG ---
# start_time = time.time()

# # Last metadata og vektorer
# vectors = np.load("vectors.npy").astype("float16")  # sparer RAM
# with open("metadata.json", "r", encoding="utf-8") as f:
#     metadata = json.load(f)

# # Lag FAISS-indeks (cosine similarity via inner product p√• normaliserte vektorer)
# dim = vectors.shape[1]
# index = faiss.IndexFlatIP(dim)
# index.add(vectors)

# # Last inn embedding-modellen
# model = SentenceTransformer("all-MiniLM-L6-v2")

# loading_time = time.time() - start_time
# print(f"‚è±Ô∏è Lasting av modell, FAISS og vektorer tok: {loading_time:.2f} sekunder\n")


# # --- FUNKSJON FOR √Ö S√òKE LOVTEKST ---
# def finn_lovtekst(query, topp_n=3):
#     search_start = time.time()

#     # Embed sp√∏rringen
#     query_vec = model.encode([query], convert_to_numpy=True)
#     query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)
#     query_vec = query_vec.astype("float16")  # matcher vektorene

#     # S√∏k i FAISS
#     scores, ids = index.search(query_vec, topp_n)

#     relevant_docs = []
#     for i, idx in enumerate(ids[0]):
#         doc = metadata[idx].copy()
#         doc["similarity_score"] = float(scores[0][i])
#         relevant_docs.append(doc)

#     # Vis resultater
#     for i, idx in enumerate(ids[0]):
#         doc = metadata[idx]
#         print(f"\nüîç Treff #{i+1} (score: {scores[0][i]:.4f})")
#         print(f"Tittel: {doc['title']}")
#         print(f"Kapittel: {doc['chapter']}")
#         print(f"Tekst: {doc['text'][:500]}...")  # f√∏rste 500 tegn

#     search_time = time.time() - search_start
#     print(f"\n‚è±Ô∏è S√∏ket tok: {search_time:.4f} sekunder")

#     return relevant_docs


# # --- EKSEMPEL P√Ö BRUK ---
# if __name__ == "__main__":
#     query = "Annet ledd"
#     treff = finn_lovtekst(query, topp_n=3)
# import numpy as np
# import json
# import faiss
# from sentence_transformers import SentenceTransformer

# def finn_lovtekst(query, vectors_file="vectors.npy", metadata_file="metadata.json",
#                   embedding_model_name="all-MiniLM-L6-v2", topp_n=1):
#     # Last inn embeddings og metadata
#     vectors = np.load(vectors_file)
#     with open(metadata_file, "r", encoding="utf-8") as f:
#         metadata = json.load(f)

#     # Lag FAISS-indeks (bruker cosine similarity via inner product p√• normaliserte vektorer)
#     dim = vectors.shape[1]
#     index = faiss.IndexFlatIP(dim)
#     index.add(vectors)

#     # Embed sp√∏rringen
#     model = SentenceTransformer(embedding_model_name)
#     query_vec = model.encode([query], convert_to_numpy=True)
#     query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)

#     # S√∏k etter mest lignende
#     scores, ids = index.search(query_vec, topp_n)

#     relevant_docs = []
#     for i, idx in enumerate(ids[0]):
#         doc = metadata[idx].copy()
#         doc["similarity_score"] = float(scores[0][i])  # legg til score for referanse
#         relevant_docs.append(doc)

#     # Vis resultat(er)
#     for i, idx in enumerate(ids[0]):
#         doc = metadata[idx]
#         print(f"\nüîç Treff #{i+1} (score: {scores[0][i]:.4f})")
#         print(f"Tittel: {doc['title']}")
#         print(f"Kapittel: {doc['chapter']}")
#         print(f"Tekst: {doc['text'][:500]}...")  # Vis f√∏rste 500 tegn

#     return relevant_docs

# # if __name__ == "__main__":
# #     tekst = input("Skriv inn en tekst du vil sammenligne med lovene:\n> ")
# #     finn_lovtekst(tekst)
# import time
# import numpy as np
# import faiss
# import json
# from sentence_transformers import SentenceTransformer

# # --- Timer start for lasting ---
# t0 = time.time()

# # Last inn FAISS-indeks
# index = faiss.read_index("laws.index")

# # Last inn metadata
# with open("metadata.json", "r", encoding="utf-8") as f:
#     metadata = json.load(f)

# # Last inn embed-modell (kun for sp√∏rring)
# model = SentenceTransformer("all-MiniLM-L6-v2")

# t1 = time.time()
# print(f"‚è±Ô∏è Lasting av FAISS, metadata og modell tok: {t1-t0:.2f} sekunder")

# # --- Funksjon for s√∏k ---
# def finn_lovtekst(query, topp_n=3):
#     # Embed sp√∏rringen
#     query_vec = model.encode([query], convert_to_numpy=True)
#     query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)

#     # S√∏k
#     t_search_start = time.time()
#     scores, ids = index.search(query_vec, topp_n)
#     t_search_end = time.time()
#     print(f"‚è±Ô∏è S√∏ket tok: {t_search_end - t_search_start:.4f} sekunder")

#     # Hent relevante dokumenter
#     relevant_docs = []
#     for i, idx in enumerate(ids[0]):
#         doc = metadata[idx].copy()
#         doc["similarity_score"] = float(scores[0][i])
#         relevant_docs.append(doc)

#         # Print
#         print(f"\nüîç Treff #{i+1} (score: {scores[0][i]:.4f})")
#         print(f"Tittel: {doc['title']}")
#         print(f"Kapittel: {doc['chapter']}")
#         print(f"Tekst: {doc['text'][:500]}...")

#     return relevant_docs

# # --- Eksempel p√• bruk ---
# query = "Annet ledd"
# finn_lovtekst(query)
import numpy as np
import time

# --- Globale variabler (settes n√•r preload kj√∏rer) ---
FAISS_INDEX = None
METADATA = None
MODEL = None

def find_law(query, topp_n=3, index=None, metadata=None, model=None):
    """
    S√∏ker i lovtekst ved hjelp av FAISS og sentence-transformers.
    
    Parametre:
    - query: str, brukerens sp√∏rring
    - topp_n: int, antall treff
    - index: FAISS Index (hvis allerede lastet)
    - metadata: metadata-liste (hvis allerede lastet)
    - model: SentenceTransformer-modell (hvis allerede lastet)
    """
    global FAISS_INDEX, METADATA, MODEL

    # Bruk preload-parametre hvis tilgjengelig
    if index is None:
        index = FAISS_INDEX
    if metadata is None:
        metadata = METADATA
    if model is None:
        model = MODEL

    if index is None or metadata is None or model is None:
        raise ValueError("FAISS, metadata og/eller modell er ikke lastet.")

    # --- Embed sp√∏rring ---
    query_vec = model.encode([query], convert_to_numpy=True)
    query_vec = query_vec / np.linalg.norm(query_vec, axis=1, keepdims=True)

    # --- S√∏k ---
    t0 = time.time()
    scores, ids = index.search(query_vec, topp_n)
    t1 = time.time()
    print(f"‚è±Ô∏è S√∏ket tok: {t1-t0:.4f} sekunder")

    # --- Hent relevante dokumenter ---
    relevant_docs = []
    for i, idx in enumerate(ids[0]):
        doc = metadata[idx].copy()
        doc["similarity_score"] = float(scores[0][i])
        relevant_docs.append(doc)

        # Print
        print(f"\nüîç Treff #{i+1} (score: {scores[0][i]:.4f})")
        print(f"Tittel: {doc['title']}")
        print(f"Kapittel: {doc['chapter']}")
        print(f"Tekst: {doc['text'][:500]}...")

    return relevant_docs